# -*- coding: utf-8 -*-
"""at - games - questao 4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GsxXtZ3gK9lM7w9k4L_gBZRjS3uChOGS
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
from torchvision.datasets import CIFAR10
from torch.utils.data import DataLoader

import matplotlib.pyplot as plt

import numpy as np
import random

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(device)

# definir transformacoes de pre processamento
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalização dos dados
])

# Carregar conjunto de dados CIFAR-10
trainset = CIFAR10(root='./data', train=True, download=True, transform=transform)
testset = CIFAR10(root='./data', train=False, download=True, transform=transform)

# Definir parâmetros
batch_size = 64
num_epochs = 10
learning_rate = 0.001

# Criar data loaders
train_loader = DataLoader(dataset=trainset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(dataset=testset, batch_size=batch_size, shuffle=False)

# Definir arquitetura da CNN
class CNNModel(nn.Module):
    def __init__(self):
        super(CNNModel, self).__init__()

        self.conv_layers = nn.Sequential(
            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),   # recebe 3 canais de entrada, gera 16 canais de saída, usa um kernel de tamanho 3x3, stride de 1 e padding de 1.
            nn.ReLU(),                                              # Função de ativação ReLU aplicada após a primeira camada convolucional.
            nn.MaxPool2d(kernel_size=2, stride=2),                  # Camada de max pooling com um kernel de tamanho 2x2 e stride de 2.
            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),  # Segunda camada convolucional que recebe 16 canais de entrada (saída da camada anterior), gera 32 canais de saída, usa um kernel de tamanho 3x3, stride de 1 e padding de 1.
            nn.ReLU(),                                              # Função de ativação ReLU aplicada após a segunda camada convolucional.
            nn.MaxPool2d(kernel_size=2, stride=2)                   # Segunda camada de max pooling com um kernel de tamanho 2x2 e stride de 2.
        )

        self.fc_layers = nn.Sequential(
            nn.Linear(32 * 8 * 8, 128),                             # Camada linear que recebe um vetor de tamanho 3288 como entrada (resultado das camadas convolucionais), gera um vetor de tamanho 128
            nn.ReLU(),                                              # Função de ativação ReLU aplicada após a primeira camada linear.
            nn.Linear(128, 10)                                      # Segunda camada linear que recebe um vetor de tamanho 128 como entrada, gera um vetor de tamanho 10 (número de classes de saída).
        )

    def forward(self, x):
        x = self.conv_layers(x)                                     # passa o tensor pelas camadas convolucionais
        x = x.view(x.size(0), -1)                                   # redimensiona usando o método view
        x = self.fc_layers(x)                                       # passa o tensor pelas camadas lineares
        return x

# Inicializar o modelo e movê-lo para a GPU
model = CNNModel().to(device)

# Definir função de perda e otimizador
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

total_step = len(train_loader)

# define optimizador:
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

# define funcao de perda
criterion = nn.CrossEntropyLoss()

# Treinamento do modelo

for epoch in range(num_epochs):
    for i, (images, labels) in enumerate(train_loader):
        # Mover dados para a GPU
        images = images.to(device)
        labels = labels.to(device)

        # Forward pass
        outputs = model(images)
        loss = criterion(outputs, labels)

        # Backward e otimização
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if (i + 1) % 100 == 0:
            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'
                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))

# acuracia do modelo
model.eval()
with torch.no_grad():
    correct = 0
    total = 0
    for images, labels in test_loader:
        # Mover dados para a GPU
        images = images.to(device)
        labels = labels.to(device)

        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

    accuracy = correct / total
    print('Acurácia no conjunto de teste: {:.2f}%'.format(100 * accuracy))

# Avaliação do modelo
model.eval()
with torch.no_grad():
    correct = 0
    total = 0

    # Obter um subconjunto aleatório de imagens do conjunto de validação
    random_indices = random.sample(range(len(testset)), 6)

    fig, axs = plt.subplots(2, 3, figsize=(12, 8))  # Criação da grade com 2 linhas e 3 colunas

    for i, index in enumerate(random_indices):
        image, label = testset[index]

        # Mover dados para a GPU
        image = image.unsqueeze(0).to(device)
        label = torch.tensor([label]).to(device)

        output = model(image)
        _, predicted = torch.max(output.data, 1)

        # Desfazer a normalização da imagem
        image = image.squeeze().cpu() * 0.5 + 0.5

        # Converter imagem para numpy array
        image = np.transpose(image.numpy(), (1, 2, 0))

        # Obter a categoria correspondente
        category = testset.classes[label]
        predicted_category = testset.classes[predicted]

        # Adicionar imagem ao subplot correspondente
        ax = axs[i // 3, i % 3]  # Calcula a posição do subplot
        ax.imshow(image, interpolation='nearest')
        ax.axis('off')
        ax.set_title(f"Real: {category}\nPrevisto: {predicted_category}")

        total += 1
        if predicted == label:
            correct += 1

    plt.tight_layout()  # Ajusta o layout dos subplots
    plt.show()

    accuracy = correct / total
    print('Acurácia no conjunto de teste: {:.2f}%'.format(100 * accuracy))

# validacao rubrica de categorias
print("Classes do conjunto de dados:", trainset.classes)

last_layer = list(model.modules())[-1]
if isinstance(last_layer, torch.nn.Linear):
    num_classes = last_layer.out_features
    print("Número de classes:", num_classes)

"""Justificativa
---
* Otimizador: Adam
O otimizador Adam foi escolhido por ser uma opção popular e eficiente para otimização de redes neurais. Ele combina as vantagens do otimizador RMSprop e do otimizador de momento estocástico, adaptando as taxas de aprendizado de forma adaptativa para cada parâmetro do modelo. Isso permite um treinamento mais rápido e estável, ajudando a evitar problemas como taxas de aprendizado muito altas ou muito baixas. Além disso, o Adam possui mecanismos embutidos para lidar com gradientes esparsos e não requer ajustes manuais de taxa de aprendizado.
<br/>
* Função de perda: Cross-Entropy Loss (Entropia Cruzada)
A função de perda de Cross-Entropy Loss foi selecionada devido à sua adequação para problemas de classificação multiclasse. Ela mede a divergência entre as probabilidades previstas pelo modelo e os rótulos verdadeiros, penalizando classificações incorretas com alta confiança. A Entropia Cruzada é uma medida de desempenho comumente usada em problemas de classificação, e seu uso nesse contexto é suportado pela natureza do problema de classificação de imagens em 10 categorias distintas.
---
* A escolha do otimizador Adam e da função de perda de Cross-Entropy Loss é baseada em sua ampla aplicabilidade, eficácia comprovada e compatibilidade com o problema de classificação multiclasse.

"""